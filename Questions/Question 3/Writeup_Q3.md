# Question 3

**A TI Sitara–class** is the option I’d pick for this EtherCAT master + real-time clamp controller, mainly because it hits the “industrial timing first” sweet spot: deterministic I/O (very peripheral-friendly) + enough compute for our 1 kHz control law, without the scheduling/jitter headaches we fight on a big Linux/GPU module and without the resource constraints of a small MCU when we add EtherCAT master duties, safety interlocks, logging, and diagnostics.

If I had to justify my choice using all the data I have from the datasheet I would say: EtherCAT success is less about peak GHz and more about bounded latency + predictable scheduling. Our control loop is 1 ms nominal, and what breaks real systems is not average cycle time but the tails (rare 2-5 ms slips) that cause bad contact detection, late torque backoff, and mode-switch confirmation weirdness. Sitara-style industrial SoCs are built for this exact problem: “do Ethernet/EtherCAT + deterministic real-time tasks + basic Linux services” reliably. More specifically, For this EtherCAT controlled vise system running a 1 kHz 1 millisecond supervisory control loop I would select a TI AM625 Sitara class SoC because it provides the right balance of deterministic timing and compute margin without unnecessary overhead. The mechanical control bandwidth is only about 15 to 25 Hz so a 1 kHz loop gives more than 40 times oversampling while keeping phase lag and jitter manageable. The AM625 multi core Cortex A53 architecture allows clean isolation of the real time control thread using real time scheduling and core pinning from the EtherCAT master and logging tasks which reduces worst case latency compared to a GPU heavy Linux system on module. Unlike a 120 MHz microcontroller where EtherCAT master control and diagnostics can push CPU utilization toward CPU utilization equals 

    (C_control + C_EtherCAT + C_logging / T_cycle) 
    
approaching one the AM625 provides ample headroom while maintaining predictable scheduling. Its industrial Ethernet subsystem DMA support hardware timers and real time Linux capability make it well suited for bounded 1 millisecond cyclic communication while the servo drive continues closing the high rate inner torque loop internally. This results in a robust scalable production ready architecture optimized for determinism rather than raw clock speed.

[This is a table which compares specs ranging from utility, task partitioning/multi-threading capabilities to cost and reliability](https://docs.google.com/spreadsheets/d/17CtbpPfysyF49Yq5wPucbXkDnLgll3z3DkbJzHRhJPw/edit?gid=0#gid=0)



Ye I would use multiple threads but I would keep the actual control loop strictly single threaded and deterministic. The ControlTick function would run in one real time thread at the EtherCAT cycle rate for example 1 kHz and that thread would own all of the state such as the phase machine timers filters and setpoint generators. Anything that is not time critical like logging telemetry configuration or diagnostics would run in lower priority threads so they can never delay the control loop. For shared memory I would keep it simple and predictable. For small signals like start or process done flags I would use atomic variables like in the code. For larger data like input and output structures I would use a snapshot or double buffer approach so the control thread always works on a clean consistent copy without using blocking locks.

I would not allow multiple threads to directly access the EtherCAT communication stack. EtherCAT is cycle driven and timing sensitive and most master stacks expect a single owner thread. Allowing multiple threads to read and write to the bus can create race conditions inconsistent process data or missed deadlines. The better approach is to have one communication owner thread that handles all bus access and updates a process image. The control thread reads from a stable input snapshot and writes its commands to an output buffer which the communication layer then transmits on the next cycle. Any other threads only read copies of the data and never touch the live bus structures.

For version control I would use Git with a hosted platform such as GitHub or GitLab. We would keep a clean separation between the control logic and the communication stack either as separate modules in the same repository or clearly separated components. Development would happen in feature branches with pull requests and code reviews before merging into the main branch. Continuous integration would automatically build the code and run tests on every commit. We would tag stable releases using semantic versioning so both the control code and the communication stack remain compatible and traceable. This keeps collaboration structured and prevents one layer from accidentally breaking the other.
